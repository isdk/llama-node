import path from "path";
import { fileURLToPath } from "url";
import { downloadFile, downloadSequence } from "ipull";
import fs from "fs-extra";
import chalk from "chalk";
import withStatusLogs from "../../src/utils/withStatusLogs.js";
import { withLockfile } from "../../src/utils/withLockfile.js";
import { isCI } from "../../src/config.js";

const __dirname = path.dirname(fileURLToPath(import.meta.url));

const modelsFolder = path.join(__dirname, "..", ".models");
const supportedModels = {
    "qwen2.5-1.5b-instruct.Q4_0.gguf": "https://huggingface.co/bartowski/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/Qwen2.5-1.5B-Instruct-Q4_0.gguf?download=true",
    "gemma-2-2b-it.Q4_K_M.gguf": "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q4_K_M.gguf?download=true",
    "stable-code-3b-Q5_K_M.gguf": "https://huggingface.co/stabilityai/stable-code-3b/resolve/main/stable-code-3b-Q5_K_M.gguf?download=true",
    "bge-small-en-v1.5-q8_0.gguf": "https://huggingface.co/CompendiumLabs/bge-small-en-v1.5-gguf/resolve/main/bge-small-en-v1.5-q8_0.gguf?download=true",
    "Meta-Llama-3-8B-Instruct-Q4_K_M.gguf": "https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf?download=true",
    "lora-Llama-3-Instruct-abliteration-LoRA-8B-f16.gguf": "https://huggingface.co/ngxson/test_gguf_lora_adapter/resolve/main/lora-Llama-3-Instruct-abliteration-LoRA-8B-f16.gguf?download=true",
    "Meta-Llama-3.1-8B-Instruct.Q4_K_M.gguf": "https://huggingface.co/mradermacher/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct.Q4_K_M.gguf?download=true",
    "codegemma-2b-Q4_K_M.gguf": "https://huggingface.co/bartowski/codegemma-2b-GGUF/resolve/main/codegemma-2b-Q4_K_M.gguf?download=true",
    "Llama-3.2-3B-Instruct.Q4_K_M.gguf": "https://huggingface.co/mradermacher/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct.Q4_K_M.gguf?download=true",
    "nomic-embed-text-v1.5.Q4_K_M.gguf": "https://huggingface.co/nomic-ai/nomic-embed-text-v1.5-GGUF/resolve/main/nomic-embed-text-v1.5.Q4_K_M.gguf?download=true",
    "bge-reranker-v2-m3-Q8_0.gguf": "https://huggingface.co/gpustack/bge-reranker-v2-m3-GGUF/resolve/main/bge-reranker-v2-m3-Q8_0.gguf?download=true"
} as const;

export const modelGroups = {
    "essential": [
        "qwen2.5-1.5b-instruct.Q4_0.gguf",
        "bge-small-en-v1.5-q8_0.gguf",
        "nomic-embed-text-v1.5.Q4_K_M.gguf",
        "bge-reranker-v2-m3-Q8_0.gguf"
    ],
    "large": [
        "Meta-Llama-3-8B-Instruct-Q4_K_M.gguf",
        "lora-Llama-3-Instruct-abliteration-LoRA-8B-f16.gguf",
        "Meta-Llama-3.1-8B-Instruct.Q4_K_M.gguf",
        "Llama-3.2-3B-Instruct.Q4_K_M.gguf",
        "gemma-2-2b-it.Q4_K_M.gguf",
        "codegemma-2b-Q4_K_M.gguf",
        "stable-code-3b-Q5_K_M.gguf"
    ]
} as const;

export async function getModelFile(modelName: keyof typeof supportedModels) {
    if (supportedModels[modelName] == null)
        throw new Error(`Model "${modelName}" is not supported`);

    const modelFilePath = path.join(modelsFolder, modelName);

    if (await fs.pathExists(modelFilePath))
        return modelFilePath;

    await fs.ensureDir(modelsFolder);

    return await withStatusLogs({
        loading: chalk.blue(`Downloading model "${modelName}"`),
        success: chalk.blue(`Downloaded model "${modelName}"`),
        fail: chalk.blue(`Failed to download model "${modelName}"`)
    }, async () => {
        return await withLockfile({
            resourcePath: modelFilePath
        }, async () => {
            const modelUrl = supportedModels[modelName];

            const downloader = await downloadFile({
                url: modelUrl,
                directory: path.dirname(modelFilePath),
                fileName: path.basename(modelFilePath),
                cliProgress: true,
                cliStyle: isCI ? "ci" : "fancy"
            });
            await downloader.download();

            return modelFilePath;
        });
    });
}

export async function downloadAllModels(options: {
    groups?: (keyof typeof modelGroups)[]
} = {}) {
    const existingModels = new Set<string>();
    const pendingDownloads: ReturnType<typeof downloadFile>[] = [];

    const modelsToDownload = new Set<keyof typeof supportedModels>();

    if (options.groups != null && options.groups.length > 0) {
        for (const group of options.groups) {
            const models = modelGroups[group];
            if (models == null)
                throw new Error(`Model group "${group}" is not supported`);

            for (const modelName of models) {
                modelsToDownload.add(modelName as keyof typeof supportedModels);
            }
        }
    } else {
        for (const modelName of Object.keys(supportedModels)) {
            modelsToDownload.add(modelName as keyof typeof supportedModels);
        }
    }

    for (const modelName of modelsToDownload) {
        if (supportedModels[modelName] == null)
            continue;

        const modelFilePath = path.join(modelsFolder, modelName);

        if (await fs.pathExists(modelFilePath)) {
            existingModels.add(modelName);
            continue;
        }

        const modelUrl = supportedModels[modelName];
        pendingDownloads.push(
            downloadFile({
                url: modelUrl,
                directory: path.dirname(modelFilePath),
                fileName: path.basename(modelFilePath)
            })
        );
    }

    if (existingModels.size > 0) {
        if (pendingDownloads.length === 0)
            console.info("All models are already downloaded");
        else
            console.info(`Already downloaded ${existingModels.size} model${existingModels.size === 1 ? "" : "s"}\n`);
    }

    if (pendingDownloads.length > 0) {
        console.info(`Downloading ${pendingDownloads.length} model${pendingDownloads.length === 1 ? "" : "s"}`);
        const downloader = await downloadSequence({
            cliProgress: true,
            cliStyle: isCI ? "ci" : "fancy",
            parallelDownloads: 4
        }, ...pendingDownloads);
        await downloader.download();
    }
}
