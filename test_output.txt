
> @isdk/llama-node@0.1.0 test:modelDependent
> vitest run ./test/modelDependent


 RUN  v3.2.4 /home/riceball/dev/ai/gpt/libs_c/node-llama.node/new_arch

stderr | test/modelDependent/llama3.2/sequenceState.test.ts > llama 3.2 > chatSession > sequence state > restoring to a smaller context sequence fails - 2 sequences
[node-llama-cpp] llama_context: n_ctx is not divisible by n_seq_max - rounding down to 512

 ‚ùØ test/modelDependent/llama3.2/sequenceState.test.ts (4 tests | 4 failed) 7061ms
   √ó llama 3.2 > chatSession > sequence state > save and load a state works properly 2066ms
     ‚Üí LlamaChatSession is not a constructor
   √ó llama 3.2 > chatSession > sequence state > save and load a state works across different contexts 1635ms
     ‚Üí LlamaChatSession is not a constructor
   √ó llama 3.2 > chatSession > sequence state > restoring to a smaller context sequence fails 1777ms
     ‚Üí LlamaChatSession is not a constructor
   √ó llama 3.2 > chatSession > sequence state > restoring to a smaller context sequence fails - 2 sequences 1582ms
     ‚Üí LlamaChatSession is not a constructor
stderr | test/modelDependent/model.test.ts
[node-llama-cpp] load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect

[
    {
        "name": "English",
        "value": "en"
    },
    {
        "name": "Spanish",
        "value": "es"
    },
    {
        "name": "French",
        "value": "fr"
    },
    {
        "name": "German",
        "value": "de"
    },
    {
        "name": "Italian",
        "value": "it"
    },
    {
        "name": "Portuguese",
        "value": "pt"
    },
    {
        "name": "Chinese",
        "value": "zh"
    },
    {
        "name": "Japanese",
        "value": "ja"
    },
    {
        "name": "Korean",
        "value": "ko"
    },
    {
        "name": "Russian",
        "value": "ru"
    }
]4 
stderr | test/modelDependent/model.test.ts
[node-llama-cpp] llama_kv_cache_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)

 ‚ùØ test/modelDependent/model.test.ts (3 tests | 1 failed) 6609ms
   √ó original completionSync vs LlamaCompletion > should same result 6420ms
     ‚Üí expected '[\n    {\n        "name": "English",\‚Ä¶' to be '[\n    {\n        "name": "English",\‚Ä¶' // Object.is equality
   ‚úì LlamaModel > should deTokenizePiece 1ms
   ‚úì LlamaModel > should completionSync 186ms
 ‚ùØ test/modelDependent/qwen3-0.6b/reasoningBudget.test.ts (1 test | 1 failed) 1126ms
   √ó qwen3 0.6b > reasoning budget > doesn't exceed reasoning budget 1125ms
     ‚Üí LlamaChatSession is not a constructor
 ‚ùØ test/modelDependent/llama3.1/tokenBias.test.ts (1 test | 1 failed) 2401ms
   √ó llama 3.1 > token bias > say a word 2401ms
     ‚Üí LlamaChatSession is not a constructor
 ‚ùØ test/modelDependent/llama3.1/tokenPredictor.test.ts (3 tests | 1 failed | 2 skipped) 2420ms
   ‚Üì llama 3.1 > token predictor > DraftModelTokenPredictor 24ms
   √ó llama 3.1 > token predictor > InputLookupTokenPredictor > no evaluation 2395ms
     ‚Üí LlamaChatSession is not a constructor
   ‚Üì llama 3.1 > token predictor > InputLookupTokenPredictor > with evaluation
 ‚ùØ test/modelDependent/llama3.2/completion.test.ts (1 test | 1 failed) 1745ms
   √ó llama 3.2 > chatSession > resolved to the correct chat wrapper 1745ms
     ‚Üí LlamaChatSession is not a constructor
 ‚ùØ test/modelDependent/llama3.2/promptCompletion.test.ts (1 test | 1 failed) 1740ms
   √ó llama 3.2 > prompt completion > prompt completion isn't kept in the next evaluation 1740ms
     ‚Üí resolveChatWrapper is not a function
 ‚ùØ test/modelDependent/stableCode/stableCodeModelGpuLayersOptions.test.ts (8 tests | 5 failed) 393ms
   ‚úì stableCode > model options > Resolve the correct number of GPU layers > attempts to resolve 0 gpuLayers 18ms
   √ó stableCode > model options > Resolve the correct number of GPU layers > attempts to resolve 16 gpuLayers 8ms
     ‚Üí Snapshot `stableCode > model options > Resolve the correct number of GPU layers > attempts to resolve 16 gpuLayers 1` mismatched
   √ó stableCode > model options > Resolve the correct number of GPU layers > attempts to resolve 32 gpuLayers 10ms
     ‚Üí Snapshot `stableCode > model options > Resolve the correct number of GPU layers > attempts to resolve 32 gpuLayers 1` mismatched
   ‚úì stableCode > model options > Resolve the correct number of GPU layers > attempts to resolve 33 gpuLayers 12ms
   ‚úì stableCode > model options > Resolve the correct number of GPU layers > attempts to resolve "max" 55ms
   √ó stableCode > model options > Resolve the correct number of GPU layers > attempts to resolve "auto" 123ms
     ‚Üí Snapshot `stableCode > model options > Resolve the correct number of GPU layers > attempts to resolve "auto" 4` mismatched
   √ó stableCode > model options > Resolve the correct number of GPU layers > attempts to resolve {min?: number, max?: number} 97ms
     ‚Üí Snapshot `stableCode > model options > Resolve the correct number of GPU layers > attempts to resolve {min?: number, max?: number} 5` mismatched
   √ó stableCode > model options > Resolve the correct number of GPU layers > attempts to resolve {fitContext?: {contextSize?: number}} 69ms
     ‚Üí Snapshot `stableCode > model options > Resolve the correct number of GPU layers > attempts to resolve {fitContext?: {contextSize?: number}} 5` mismatched
 ‚úì test/modelDependent/llama3/lora.test.ts (4 tests) 9740ms
   ‚úì llama 3 > lora > dispose context unloads lora  2411ms
   ‚úì llama 3 > lora > using multiple contexts with lora  2497ms
   ‚úì llama 3 > lora > unload model unloads lora  2480ms
   ‚úì llama 3 > lora > implicitly unloading model and context with lora  2351ms
(node:44806) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 exit listeners added to [process]. MaxListeners is 10. Use emitter.setMaxListeners() to increase limit
(Use `node --trace-warnings ...` to show where the warning was created)
stderr | test/modelDependent/codegemma/parallel.test.ts > CodeGemma > parallel > can use multiple bindings in parallel
[node-llama-cpp] load: control-looking token:    107 '<end_of_turn>' was not control-type; this is probably a bug in the model. its type will be overridden

stderr | test/modelDependent/codegemma/parallel.test.ts > CodeGemma > parallel > can use multiple bindings in parallel
[node-llama-cpp] load: control-looking token:     68 '<|fim_middle|>' was not control-type; this is probably a bug in the model. its type will be overridden

stderr | test/modelDependent/codegemma/parallel.test.ts > CodeGemma > parallel > can use multiple bindings in parallel
[node-llama-cpp] load: control-looking token:     69 '<|fim_suffix|>' was not control-type; this is probably a bug in the model. its type will be overridden

stderr | test/modelDependent/codegemma/parallel.test.ts > CodeGemma > parallel > can use multiple bindings in parallel
[node-llama-cpp] load: control-looking token:     67 '<|fim_prefix|>' was not control-type; this is probably a bug in the model. its type will be overridden

stderr | test/modelDependent/codegemma/parallel.test.ts > CodeGemma > parallel > can use multiple bindings in parallel
[node-llama-cpp] load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect

stderr | test/modelDependent/codegemma/parallel.test.ts > CodeGemma > parallel > can use multiple bindings in parallel
[node-llama-cpp] load: control-looking token:    107 '<end_of_turn>' was not control-type; this is probably a bug in the model. its type will be overridden

stderr | test/modelDependent/codegemma/parallel.test.ts > CodeGemma > parallel > can use multiple bindings in parallel
[node-llama-cpp] load: control-looking token:     68 '<|fim_middle|>' was not control-type; this is probably a bug in the model. its type will be overridden

stderr | test/modelDependent/codegemma/parallel.test.ts > CodeGemma > parallel > can use multiple bindings in parallel
[node-llama-cpp] load: control-looking token:     69 '<|fim_suffix|>' was not control-type; this is probably a bug in the model. its type will be overridden

stderr | test/modelDependent/codegemma/parallel.test.ts > CodeGemma > parallel > can use multiple bindings in parallel
[node-llama-cpp] load: control-looking token:     67 '<|fim_prefix|>' was not control-type; this is probably a bug in the model. its type will be overridden

stderr | test/modelDependent/codegemma/parallel.test.ts > CodeGemma > parallel > can use multiple bindings in parallel
[node-llama-cpp] load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect

stderr | test/modelDependent/codegemma/parallel.test.ts > CodeGemma > parallel > can use multiple models in parallel
[node-llama-cpp] load: control-looking token:    107 '<end_of_turn>' was not control-type; this is probably a bug in the model. its type will be overridden

stderr | test/modelDependent/codegemma/parallel.test.ts > CodeGemma > parallel > can use multiple models in parallel
[node-llama-cpp] load: control-looking token:     68 '<|fim_middle|>' was not control-type; this is probably a bug in the model. its type will be overridden

stderr | test/modelDependent/codegemma/parallel.test.ts > CodeGemma > parallel > can use multiple models in parallel
[node-llama-cpp] load: control-looking token:     69 '<|fim_suffix|>' was not control-type; this is probably a bug in the model. its type will be overridden

stderr | test/modelDependent/codegemma/parallel.test.ts > CodeGemma > parallel > can use multiple models in parallel
[node-llama-cpp] load: control-looking token:     67 '<|fim_prefix|>' was not control-type; this is probably a bug in the model. its type will be overridden

stderr | test/modelDependent/codegemma/parallel.test.ts > CodeGemma > parallel > can use multiple models in parallel
[node-llama-cpp] load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect

stderr | test/modelDependent/codegemma/parallel.test.ts > CodeGemma > parallel > can use multiple models in parallel
[node-llama-cpp] load: control-looking token:    107 '<end_of_turn>' was not control-type; this is probably a bug in the model. its type will be overridden

stderr | test/modelDependent/codegemma/parallel.test.ts > CodeGemma > parallel > can use multiple models in parallel
[node-llama-cpp] load: control-looking token:     68 '<|fim_middle|>' was not control-type; this is probably a bug in the model. its type will be overridden

stderr | test/modelDependent/codegemma/parallel.test.ts > CodeGemma > parallel > can use multiple models in parallel
[node-llama-cpp] load: control-looking token:     69 '<|fim_suffix|>' was not control-type; this is probably a bug in the model. its type will be overridden

stderr | test/modelDependent/codegemma/parallel.test.ts > CodeGemma > parallel > can use multiple models in parallel
[node-llama-cpp] load: control-looking token:     67 '<|fim_prefix|>' was not control-type; this is probably a bug in the model. its type will be overridden

stderr | test/modelDependent/codegemma/parallel.test.ts > CodeGemma > parallel > can use multiple models in parallel
[node-llama-cpp] load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect

stderr | test/modelDependent/codegemma/parallel.test.ts > CodeGemma > parallel > can use multiple contexts in parallel
[node-llama-cpp] load: control-looking token:    107 '<end_of_turn>' was not control-type; this is probably a bug in the model. its type will be overridden

stderr | test/modelDependent/codegemma/parallel.test.ts > CodeGemma > parallel > can use multiple contexts in parallel
[node-llama-cpp] load: control-looking token:     68 '<|fim_middle|>' was not control-type; this is probably a bug in the model. its type will be overridden

stderr | test/modelDependent/codegemma/parallel.test.ts > CodeGemma > parallel > can use multiple contexts in parallel
[node-llama-cpp] load: control-looking token:     69 '<|fim_suffix|>' was not control-type; this is probably a bug in the model. its type will be overridden

stderr | test/modelDependent/codegemma/parallel.test.ts > CodeGemma > parallel > can use multiple contexts in parallel
[node-llama-cpp] load: control-looking token:     67 '<|fim_prefix|>' was not control-type; this is probably a bug in the model. its type will be overridden

stderr | test/modelDependent/codegemma/parallel.test.ts > CodeGemma > parallel > can use multiple contexts in parallel
[node-llama-cpp] load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect

stderr | test/modelDependent/codegemma/parallel.test.ts > CodeGemma > parallel > can use multiple context sequences in parallel
[node-llama-cpp] load: control-looking token:    107 '<end_of_turn>' was not control-type; this is probably a bug in the model. its type will be overridden

stderr | test/modelDependent/codegemma/parallel.test.ts > CodeGemma > parallel > can use multiple context sequences in parallel
[node-llama-cpp] load: control-looking token:     68 '<|fim_middle|>' was not control-type; this is probably a bug in the model. its type will be overridden

stderr | test/modelDependent/codegemma/parallel.test.ts > CodeGemma > parallel > can use multiple context sequences in parallel
[node-llama-cpp] load: control-looking token:     69 '<|fim_suffix|>' was not control-type; this is probably a bug in the model. its type will be overridden

stderr | test/modelDependent/codegemma/parallel.test.ts > CodeGemma > parallel > can use multiple context sequences in parallel
[node-llama-cpp] load: control-looking token:     67 '<|fim_prefix|>' was not control-type; this is probably a bug in the model. its type will be overridden

stderr | test/modelDependent/codegemma/parallel.test.ts > CodeGemma > parallel > can use multiple context sequences in parallel
[node-llama-cpp] load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect

 ‚úì test/modelDependent/codegemma/parallel.test.ts (4 tests) 10718ms
   ‚úì CodeGemma > parallel > can use multiple bindings in parallel  3574ms
   ‚úì CodeGemma > parallel > can use multiple models in parallel  3379ms
   ‚úì CodeGemma > parallel > can use multiple contexts in parallel  2008ms
   ‚úì CodeGemma > parallel > can use multiple context sequences in parallel  1755ms
stderr | test/modelDependent/stableCode/completion.test.ts > stableCode > completion > complete a series
[node-llama-cpp] load: missing pre-tokenizer type, using: 'default'

stderr | test/modelDependent/stableCode/completion.test.ts > stableCode > completion > complete a series
[node-llama-cpp] load:                                             

stderr | test/modelDependent/stableCode/completion.test.ts > stableCode > completion > complete a series
[node-llama-cpp] load: ************************************        

stderr | test/modelDependent/stableCode/completion.test.ts > stableCode > completion > complete a series
[node-llama-cpp] load: GENERATION QUALITY WILL BE DEGRADED!        

stderr | test/modelDependent/stableCode/completion.test.ts > stableCode > completion > complete a series
[node-llama-cpp] load: CONSIDER REGENERATING THE MODEL             

stderr | test/modelDependent/stableCode/completion.test.ts > stableCode > completion > complete a series
[node-llama-cpp] load: ************************************        

stderr | test/modelDependent/stableCode/completion.test.ts > stableCode > completion > complete a series
[node-llama-cpp] load:                                             

stderr | test/modelDependent/stableCode/completion.test.ts > stableCode > completion > complete pretictable text
[node-llama-cpp] load: missing pre-tokenizer type, using: 'default'

stderr | test/modelDependent/stableCode/completion.test.ts > stableCode > completion > complete pretictable text
[node-llama-cpp] load:                                             

stderr | test/modelDependent/stableCode/completion.test.ts > stableCode > completion > complete pretictable text
[node-llama-cpp] load: ************************************        

stderr | test/modelDependent/stableCode/completion.test.ts > stableCode > completion > complete pretictable text
[node-llama-cpp] load: GENERATION QUALITY WILL BE DEGRADED!        

stderr | test/modelDependent/stableCode/completion.test.ts > stableCode > completion > complete pretictable text
[node-llama-cpp] load: CONSIDER REGENERATING THE MODEL             

stderr | test/modelDependent/stableCode/completion.test.ts > stableCode > completion > complete pretictable text
[node-llama-cpp] load: ************************************        

stderr | test/modelDependent/stableCode/completion.test.ts > stableCode > completion > complete pretictable text
[node-llama-cpp] load:                                             

stderr | test/modelDependent/stableCode/completion.test.ts > stableCode > infill > fill the gap in a series
[node-llama-cpp] load: missing pre-tokenizer type, using: 'default'

stderr | test/modelDependent/stableCode/completion.test.ts > stableCode > infill > fill the gap in a series
[node-llama-cpp] load:                                             

stderr | test/modelDependent/stableCode/completion.test.ts > stableCode > infill > fill the gap in a series
[node-llama-cpp] load: ************************************        

stderr | test/modelDependent/stableCode/completion.test.ts > stableCode > infill > fill the gap in a series
[node-llama-cpp] load: GENERATION QUALITY WILL BE DEGRADED!        

stderr | test/modelDependent/stableCode/completion.test.ts > stableCode > infill > fill the gap in a series
[node-llama-cpp] load: CONSIDER REGENERATING THE MODEL             

stderr | test/modelDependent/stableCode/completion.test.ts > stableCode > infill > fill the gap in a series
[node-llama-cpp] load: ************************************        

stderr | test/modelDependent/stableCode/completion.test.ts > stableCode > infill > fill the gap in a series
[node-llama-cpp] load:                                             

stderr | test/modelDependent/stableCode/completion.test.ts > stableCode > infill > fill expected text
[node-llama-cpp] load: missing pre-tokenizer type, using: 'default'

stderr | test/modelDependent/stableCode/completion.test.ts > stableCode > infill > fill expected text
[node-llama-cpp] load:                                             

stderr | test/modelDependent/stableCode/completion.test.ts > stableCode > infill > fill expected text
[node-llama-cpp] load: ************************************        

stderr | test/modelDependent/stableCode/completion.test.ts > stableCode > infill > fill expected text
[node-llama-cpp] load: GENERATION QUALITY WILL BE DEGRADED!        

stderr | test/modelDependent/stableCode/completion.test.ts > stableCode > infill > fill expected text
[node-llama-cpp] load: CONSIDER REGENERATING THE MODEL             

stderr | test/modelDependent/stableCode/completion.test.ts > stableCode > infill > fill expected text
[node-llama-cpp] load: ************************************        

stderr | test/modelDependent/stableCode/completion.test.ts > stableCode > infill > fill expected text
[node-llama-cpp] load:                                             

 ‚úì test/modelDependent/stableCode/completion.test.ts (4 tests) 3964ms
   ‚úì stableCode > completion > complete a series  1077ms
   ‚úì stableCode > completion > complete pretictable text  985ms
   ‚úì stableCode > infill > fill the gap in a series  996ms
   ‚úì stableCode > infill > fill expected text  905ms
stderr | test/modelDependent/bgeReranker/rank.test.ts > bgeReranker > rank > rank and sort without scores
[node-llama-cpp] load: model vocab missing newline token, using special_pad_id instead

stderr | test/modelDependent/bgeReranker/rank.test.ts > bgeReranker > rank > rank and sort without scores
[node-llama-cpp] load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect

stderr | test/modelDependent/bgeReranker/rank.test.ts > bgeReranker > rank > rank and sort without scores
[node-llama-cpp] llama_init_from_model: model default pooling_type is [-1], but [4] was specified

 ‚úì test/modelDependent/bgeReranker/rank.test.ts (4 tests | 3 skipped) 1896ms
   ‚úì bgeReranker > rank > rank and sort without scores  1894ms
stderr | test/modelDependent/codegemma/completion.test.ts > CodeGemma > completion > complete a list of sweet fruits
[node-llama-cpp] load: control-looking token:    107 '<end_of_turn>' was not control-type; this is probably a bug in the model. its type will be overridden

stderr | test/modelDependent/codegemma/completion.test.ts > CodeGemma > completion > complete a list of sweet fruits
[node-llama-cpp] load: control-looking token:     68 '<|fim_middle|>' was not control-type; this is probably a bug in the model. its type will be overridden

stderr | test/modelDependent/codegemma/completion.test.ts > CodeGemma > completion > complete a list of sweet fruits
[node-llama-cpp] load: control-looking token:     69 '<|fim_suffix|>' was not control-type; this is probably a bug in the model. its type will be overridden

stderr | test/modelDependent/codegemma/completion.test.ts > CodeGemma > completion > complete a list of sweet fruits
[node-llama-cpp] load: control-looking token:     67 '<|fim_prefix|>' was not control-type; this is probably a bug in the model. its type will be overridden

stderr | test/modelDependent/codegemma/completion.test.ts > CodeGemma > completion > complete a list of sweet fruits
[node-llama-cpp] load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect

stderr | test/modelDependent/codegemma/completion.test.ts > CodeGemma > infill > fill in a list of sweet fruits
[node-llama-cpp] load: control-looking token:    107 '<end_of_turn>' was not control-type; this is probably a bug in the model. its type will be overridden

stderr | test/modelDependent/codegemma/completion.test.ts > CodeGemma > infill > fill in a list of sweet fruits
[node-llama-cpp] load: control-looking token:     68 '<|fim_middle|>' was not control-type; this is probably a bug in the model. its type will be overridden

stderr | test/modelDependent/codegemma/completion.test.ts > CodeGemma > infill > fill in a list of sweet fruits
[node-llama-cpp] load: control-looking token:     69 '<|fim_suffix|>' was not control-type; this is probably a bug in the model. its type will be overridden

stderr | test/modelDependent/codegemma/completion.test.ts > CodeGemma > infill > fill in a list of sweet fruits
[node-llama-cpp] load: control-looking token:     67 '<|fim_prefix|>' was not control-type; this is probably a bug in the model. its type will be overridden

stderr | test/modelDependent/codegemma/completion.test.ts > CodeGemma > infill > fill in a list of sweet fruits
[node-llama-cpp] load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect

 ‚úì test/modelDependent/codegemma/completion.test.ts (2 tests) 3272ms
   ‚úì CodeGemma > completion > complete a list of sweet fruits  1751ms
   ‚úì CodeGemma > infill > fill in a list of sweet fruits  1521ms
 ‚úì test/modelDependent/llama3.1/completion.test.ts (1 test) 2584ms
   ‚úì llama 3.1 > completion > complete a list of sweet fruits  2584ms
 ‚úì test/modelDependent/llama3.1/evaluateWithMetadata.test.ts (5 tests | 4 skipped) 2688ms
   ‚úì llama 3.1 > evaluate with metadata > no options  2686ms
stderr | test/modelDependent/stableCode/asyncModelLoad.test.ts > stableCode > async model load > load asynchronously
[node-llama-cpp] load: missing pre-tokenizer type, using: 'default'

stderr | test/modelDependent/stableCode/asyncModelLoad.test.ts > stableCode > async model load > load asynchronously
[node-llama-cpp] load:                                             

stderr | test/modelDependent/stableCode/asyncModelLoad.test.ts > stableCode > async model load > load asynchronously
[node-llama-cpp] load: ************************************        

stderr | test/modelDependent/stableCode/asyncModelLoad.test.ts > stableCode > async model load > load asynchronously
[node-llama-cpp] load: GENERATION QUALITY WILL BE DEGRADED!        

stderr | test/modelDependent/stableCode/asyncModelLoad.test.ts > stableCode > async model load > load asynchronously
[node-llama-cpp] load: CONSIDER REGENERATING THE MODEL             

stderr | test/modelDependent/stableCode/asyncModelLoad.test.ts > stableCode > async model load > load asynchronously
[node-llama-cpp] load: ************************************        

stderr | test/modelDependent/stableCode/asyncModelLoad.test.ts > stableCode > async model load > load asynchronously
[node-llama-cpp] load:                                             

stderr | test/modelDependent/stableCode/asyncModelLoad.test.ts > stableCode > async model load > load progress emitted
[node-llama-cpp] load: missing pre-tokenizer type, using: 'default'

stderr | test/modelDependent/stableCode/asyncModelLoad.test.ts > stableCode > async model load > load progress emitted
[node-llama-cpp] load:                                             

stderr | test/modelDependent/stableCode/asyncModelLoad.test.ts > stableCode > async model load > load progress emitted
[node-llama-cpp] load: ************************************        

stderr | test/modelDependent/stableCode/asyncModelLoad.test.ts > stableCode > async model load > load progress emitted
[node-llama-cpp] load: GENERATION QUALITY WILL BE DEGRADED!        

stderr | test/modelDependent/stableCode/asyncModelLoad.test.ts > stableCode > async model load > load progress emitted
[node-llama-cpp] load: CONSIDER REGENERATING THE MODEL             

stderr | test/modelDependent/stableCode/asyncModelLoad.test.ts > stableCode > async model load > load progress emitted
[node-llama-cpp] load: ************************************        

stderr | test/modelDependent/stableCode/asyncModelLoad.test.ts > stableCode > async model load > load progress emitted
[node-llama-cpp] load:                                             

stderr | test/modelDependent/stableCode/asyncModelLoad.test.ts > stableCode > async model load > abort model load works
[node-llama-cpp] load: missing pre-tokenizer type, using: 'default'

stderr | test/modelDependent/stableCode/asyncModelLoad.test.ts > stableCode > async model load > abort model load works
[node-llama-cpp] load:                                             

stderr | test/modelDependent/stableCode/asyncModelLoad.test.ts > stableCode > async model load > abort model load works
[node-llama-cpp] load: ************************************        

stderr | test/modelDependent/stableCode/asyncModelLoad.test.ts > stableCode > async model load > abort model load works
[node-llama-cpp] load: GENERATION QUALITY WILL BE DEGRADED!        

stderr | test/modelDependent/stableCode/asyncModelLoad.test.ts > stableCode > async model load > abort model load works
[node-llama-cpp] load: CONSIDER REGENERATING THE MODEL             

stderr | test/modelDependent/stableCode/asyncModelLoad.test.ts > stableCode > async model load > abort model load works
[node-llama-cpp] load: ************************************        

stderr | test/modelDependent/stableCode/asyncModelLoad.test.ts > stableCode > async model load > abort model load works
[node-llama-cpp] load:                                             

 ‚úì test/modelDependent/stableCode/asyncModelLoad.test.ts (3 tests) 2485ms
   ‚úì stableCode > async model load > load asynchronously  965ms
   ‚úì stableCode > async model load > load progress emitted  846ms
   ‚úì stableCode > async model load > abort model load works  673ms
stderr | test/modelDependent/stableCode/metadataOverrides.test.ts > stableCode > metadata overrides > boolean metadata override
[node-llama-cpp] load: missing pre-tokenizer type, using: 'default'

stderr | test/modelDependent/stableCode/metadataOverrides.test.ts > stableCode > metadata overrides > boolean metadata override
[node-llama-cpp] load:                                             

stderr | test/modelDependent/stableCode/metadataOverrides.test.ts > stableCode > metadata overrides > boolean metadata override
[node-llama-cpp] load: ************************************        

stderr | test/modelDependent/stableCode/metadataOverrides.test.ts > stableCode > metadata overrides > boolean metadata override
[node-llama-cpp] load: GENERATION QUALITY WILL BE DEGRADED!        

stderr | test/modelDependent/stableCode/metadataOverrides.test.ts > stableCode > metadata overrides > boolean metadata override
[node-llama-cpp] load: CONSIDER REGENERATING THE MODEL             

stderr | test/modelDependent/stableCode/metadataOverrides.test.ts > stableCode > metadata overrides > boolean metadata override
[node-llama-cpp] load: ************************************        

stderr | test/modelDependent/stableCode/metadataOverrides.test.ts > stableCode > metadata overrides > boolean metadata override
[node-llama-cpp] load:                                             

stderr | test/modelDependent/stableCode/metadataOverrides.test.ts > stableCode > metadata overrides > boolean metadata override 2
[node-llama-cpp] load: missing pre-tokenizer type, using: 'default'

stderr | test/modelDependent/stableCode/metadataOverrides.test.ts > stableCode > metadata overrides > boolean metadata override 2
[node-llama-cpp] load:                                             

stderr | test/modelDependent/stableCode/metadataOverrides.test.ts > stableCode > metadata overrides > boolean metadata override 2
[node-llama-cpp] load: ************************************        

stderr | test/modelDependent/stableCode/metadataOverrides.test.ts > stableCode > metadata overrides > boolean metadata override 2
[node-llama-cpp] load: GENERATION QUALITY WILL BE DEGRADED!        

stderr | test/modelDependent/stableCode/metadataOverrides.test.ts > stableCode > metadata overrides > boolean metadata override 2
[node-llama-cpp] load: CONSIDER REGENERATING THE MODEL             

stderr | test/modelDependent/stableCode/metadataOverrides.test.ts > stableCode > metadata overrides > boolean metadata override 2
[node-llama-cpp] load: ************************************        

stderr | test/modelDependent/stableCode/metadataOverrides.test.ts > stableCode > metadata overrides > boolean metadata override 2
[node-llama-cpp] load:                                             

 ‚úì test/modelDependent/stableCode/metadataOverrides.test.ts (2 tests) 1729ms
   ‚úì stableCode > metadata overrides > boolean metadata override  888ms
   ‚úì stableCode > metadata overrides > boolean metadata override 2  840ms
stderr | test/modelDependent/nomicEmbedText/embedding.test.ts > nomic embed text > embedding > deterministic
[node-llama-cpp] load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect

stderr | test/modelDependent/nomicEmbedText/embedding.test.ts > nomic embed text > embedding > deterministic
[node-llama-cpp] Using this model ("~/dev/ai/gpt/libs_c/node-llama.node/new_arch/test/.models/nomic-embed-text-v1.5.Q4_K_M.gguf") to tokenize text and then detokenize it resulted in a different text. There might be an issue with the model or the tokenizer implementation. Using this model may not work as intended

stderr | test/modelDependent/nomicEmbedText/embedding.test.ts > nomic embed text > embedding > deterministic between runs
[node-llama-cpp] load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect

stderr | test/modelDependent/nomicEmbedText/embedding.test.ts > nomic embed text > embedding > deterministic between runs
[node-llama-cpp] Using this model ("~/dev/ai/gpt/libs_c/node-llama.node/new_arch/test/.models/nomic-embed-text-v1.5.Q4_K_M.gguf") to tokenize text and then detokenize it resulted in a different text. There might be an issue with the model or the tokenizer implementation. Using this model may not work as intended

stderr | test/modelDependent/nomicEmbedText/embedding.test.ts > nomic embed text > embedding > similarity search
[node-llama-cpp] load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect

stderr | test/modelDependent/nomicEmbedText/embedding.test.ts > nomic embed text > embedding > similarity search
[node-llama-cpp] Using this model ("~/dev/ai/gpt/libs_c/node-llama.node/new_arch/test/.models/nomic-embed-text-v1.5.Q4_K_M.gguf") to tokenize text and then detokenize it resulted in a different text. There might be an issue with the model or the tokenizer implementation. Using this model may not work as intended

stderr | test/modelDependent/nomicEmbedText/embedding.test.ts > nomic embed text > embedding > similarity search 2
[node-llama-cpp] load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect

stderr | test/modelDependent/nomicEmbedText/embedding.test.ts > nomic embed text > embedding > similarity search 2
[node-llama-cpp] Using this model ("~/dev/ai/gpt/libs_c/node-llama.node/new_arch/test/.models/nomic-embed-text-v1.5.Q4_K_M.gguf") to tokenize text and then detokenize it resulted in a different text. There might be an issue with the model or the tokenizer implementation. Using this model may not work as intended

 ‚úì test/modelDependent/nomicEmbedText/embedding.test.ts (4 tests) 875ms
stderr | test/modelDependent/bge/embedding.test.ts > bge > embedding > deterministic
[node-llama-cpp] load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect

stderr | test/modelDependent/bge/embedding.test.ts > bge > embedding > deterministic
[node-llama-cpp] Using this model ("~/dev/ai/gpt/libs_c/node-llama.node/new_arch/test/.models/bge-small-en-v1.5-q8_0.gguf") to tokenize text and then detokenize it resulted in a different text. There might be an issue with the model or the tokenizer implementation. Using this model may not work as intended

stderr | test/modelDependent/bge/embedding.test.ts > bge > embedding > deterministic between runs
[node-llama-cpp] load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect

stderr | test/modelDependent/bge/embedding.test.ts > bge > embedding > deterministic between runs
[node-llama-cpp] Using this model ("~/dev/ai/gpt/libs_c/node-llama.node/new_arch/test/.models/bge-small-en-v1.5-q8_0.gguf") to tokenize text and then detokenize it resulted in a different text. There might be an issue with the model or the tokenizer implementation. Using this model may not work as intended

stderr | test/modelDependent/bge/embedding.test.ts > bge > embedding > similarity search
[node-llama-cpp] load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect

stderr | test/modelDependent/bge/embedding.test.ts > bge > embedding > similarity search
[node-llama-cpp] Using this model ("~/dev/ai/gpt/libs_c/node-llama.node/new_arch/test/.models/bge-small-en-v1.5-q8_0.gguf") to tokenize text and then detokenize it resulted in a different text. There might be an issue with the model or the tokenizer implementation. Using this model may not work as intended

stderr | test/modelDependent/bge/embedding.test.ts > bge > embedding > similarity search 2
[node-llama-cpp] load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect

stderr | test/modelDependent/bge/embedding.test.ts > bge > embedding > similarity search 2
[node-llama-cpp] Using this model ("~/dev/ai/gpt/libs_c/node-llama.node/new_arch/test/.models/bge-small-en-v1.5-q8_0.gguf") to tokenize text and then detokenize it resulted in a different text. There might be an issue with the model or the tokenizer implementation. Using this model may not work as intended

 ‚úì test/modelDependent/bge/embedding.test.ts (4 tests) 866ms
stderr | test/modelDependent/stableCode/asyncContextLoad.test.ts > stableCode > async context load > load asynchronously
[node-llama-cpp] load: missing pre-tokenizer type, using: 'default'

stderr | test/modelDependent/stableCode/asyncContextLoad.test.ts > stableCode > async context load > load asynchronously
[node-llama-cpp] load:                                             

stderr | test/modelDependent/stableCode/asyncContextLoad.test.ts > stableCode > async context load > load asynchronously
[node-llama-cpp] load: ************************************        

stderr | test/modelDependent/stableCode/asyncContextLoad.test.ts > stableCode > async context load > load asynchronously
[node-llama-cpp] load: GENERATION QUALITY WILL BE DEGRADED!        

stderr | test/modelDependent/stableCode/asyncContextLoad.test.ts > stableCode > async context load > load asynchronously
[node-llama-cpp] load: CONSIDER REGENERATING THE MODEL             

stderr | test/modelDependent/stableCode/asyncContextLoad.test.ts > stableCode > async context load > load asynchronously
[node-llama-cpp] load: ************************************        

stderr | test/modelDependent/stableCode/asyncContextLoad.test.ts > stableCode > async context load > load asynchronously
[node-llama-cpp] load:                                             

 ‚úì test/modelDependent/stableCode/asyncContextLoad.test.ts (1 test) 947ms
   ‚úì stableCode > async context load > load asynchronously  947ms
 ‚úì test/modelDependent/llama3.1/controlledEvaluate.test.ts (1 test | 1 skipped) 20ms

‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ Failed Tests 15 ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ

 FAIL  test/modelDependent/model.test.ts > original completionSync vs LlamaCompletion > should same result
AssertionError: expected '[\n    {\n        "name": "English",\‚Ä¶' to be '[\n    {\n        "name": "English",\‚Ä¶' // Object.is equality

[32m- Expected[39m
[31m+ Received[39m

[33m@@ -22,11 +22,11 @@[39m
[2m      {[22m
[2m          "name": "Portuguese",[22m
[2m          "value": "pt"[22m
[2m      },[22m
[2m      {[22m
[32m-         "name": "Chinese",[39m
[31m+         "name": "Chinese[7m (Simplified and Traditional)[27m",[39m
[2m          "value": "zh"[22m
[2m      },[22m
[2m      {[22m
[2m          "name": "Japanese",[22m
[2m          "value": "ja"[22m

 ‚ùØ test/modelDependent/model.test.ts:52:21
     50|             grammar: await llama.createGrammarForJsonSchema(schema),
     51|         });
     52|         expect(res).toBe(llamaCppResult);
       |                     ^
     53|         // contextSequence.clearHistory();
     54|         // res = await completion.generateCompletion(prompt, {

‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ[1/15]‚éØ

 FAIL  test/modelDependent/llama3.1/tokenBias.test.ts > llama 3.1 > token bias > say a word
TypeError: LlamaChatSession is not a constructor
 ‚ùØ test/modelDependent/llama3.1/tokenBias.test.ts:18:33
     16|                 contextSize: 4096
     17|             });
     18|             const chatSession = new LlamaChatSession({
       |                                 ^
     19|                 contextSequence: context.getSequence()
     20|             });

‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ[2/15]‚éØ

 FAIL  test/modelDependent/llama3.1/tokenPredictor.test.ts > llama 3.1 > token predictor > InputLookupTokenPredictor > no evaluation
TypeError: LlamaChatSession is not a constructor
 ‚ùØ test/modelDependent/llama3.1/tokenPredictor.test.ts:152:37
    150| 
    151|                 const sequence = context.getSequence();
    152|                 const chatSession = new LlamaChatSession({
       |                                     ^
    153|                     contextSequence: sequence
    154|                 });

‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ[3/15]‚éØ

 FAIL  test/modelDependent/llama3.2/completion.test.ts > llama 3.2 > chatSession > resolved to the correct chat wrapper
TypeError: LlamaChatSession is not a constructor
 ‚ùØ test/modelDependent/llama3.2/completion.test.ts:18:33
     16|                 contextSize: 4096
     17|             });
     18|             const chatSession = new LlamaChatSession({
       |                                 ^
     19|                 contextSequence: context.getSequence()
     20|             });

‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ[4/15]‚éØ

 FAIL  test/modelDependent/llama3.2/promptCompletion.test.ts > llama 3.2 > prompt completion > prompt completion isn't kept in the next evaluation
TypeError: resolveChatWrapper is not a function
 ‚ùØ test/modelDependent/llama3.2/promptCompletion.test.ts:24:30
     22|             const chatSession = new LlamaChatSession({
     23|                 contextSequence: context.getSequence(),
     24|                 chatWrapper: resolveChatWrapper(model, {
       |                              ^
     25|                     customWrapperSettings: {
     26|                         "llama3.2-lightweight": {

‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ[5/15]‚éØ

 FAIL  test/modelDependent/llama3.2/sequenceState.test.ts > llama 3.2 > chatSession > sequence state > save and load a state works properly
TypeError: LlamaChatSession is not a constructor
 ‚ùØ test/modelDependent/llama3.2/sequenceState.test.ts:26:38
     24|                 const contextSequence2 = context.getSequence();
     25| 
     26|                 const chatSession1 = new LlamaChatSession({
       |                                      ^
     27|                     contextSequence: contextSequence1
     28|                 });

‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ[6/15]‚éØ

 FAIL  test/modelDependent/llama3.2/sequenceState.test.ts > llama 3.2 > chatSession > sequence state > save and load a state works across different contexts
TypeError: LlamaChatSession is not a constructor
 ‚ùØ test/modelDependent/llama3.2/sequenceState.test.ts:138:38
    136|                 const contextSequence2 = context2.getSequence();
    137| 
    138|                 const chatSession1 = new LlamaChatSession({
       |                                      ^
    139|                     contextSequence: contextSequence1
    140|                 });

‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ[7/15]‚éØ

 FAIL  test/modelDependent/llama3.2/sequenceState.test.ts > llama 3.2 > chatSession > sequence state > restoring to a smaller context sequence fails
TypeError: LlamaChatSession is not a constructor
 ‚ùØ test/modelDependent/llama3.2/sequenceState.test.ts:199:38
    197|                 const contextSequence2 = context2.getSequence();
    198| 
    199|                 const chatSession1 = new LlamaChatSession({
       |                                      ^
    200|                     contextSequence: contextSequence1
    201|                 });

‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ[8/15]‚éØ

 FAIL  test/modelDependent/llama3.2/sequenceState.test.ts > llama 3.2 > chatSession > sequence state > restoring to a smaller context sequence fails - 2 sequences
TypeError: LlamaChatSession is not a constructor
 ‚ùØ test/modelDependent/llama3.2/sequenceState.test.ts:257:38
    255|                 const contextSequence2 = context2.getSequence();
    256| 
    257|                 const chatSession1 = new LlamaChatSession({
       |                                      ^
    258|                     contextSequence: contextSequence1
    259|                 });

‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ[9/15]‚éØ

 FAIL  test/modelDependent/qwen3-0.6b/reasoningBudget.test.ts > qwen3 0.6b > reasoning budget > doesn't exceed reasoning budget
TypeError: LlamaChatSession is not a constructor
 ‚ùØ test/modelDependent/qwen3-0.6b/reasoningBudget.test.ts:18:33
     16|                 contextSize: 512
     17|             });
     18|             const chatSession = new LlamaChatSession({
       |                                 ^
     19|                 contextSequence: context.getSequence()
     20|             });

‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ[10/15]‚éØ

 FAIL  test/modelDependent/stableCode/stableCodeModelGpuLayersOptions.test.ts > stableCode > model options > Resolve the correct number of GPU layers > attempts to resolve 16 gpuLayers
Error: Snapshot `stableCode > model options > Resolve the correct number of GPU layers > attempts to resolve 16 gpuLayers 1` mismatched

Expected: [32m"8061"[39m
Received: [31m"10748"[39m

 ‚ùØ test/modelDependent/stableCode/stableCodeModelGpuLayersOptions.test.ts:114:48
    112|                     });
    113|                     expect(res.gpuLayers).to.eql(16);
    114|                     expect(res.contextSize).to.toMatchInlineSnapshot("‚Ä¶
       |                                                ^
    115|                 }
    116|                 try {

‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ[11/15]‚éØ

 FAIL  test/modelDependent/stableCode/stableCodeModelGpuLayersOptions.test.ts > stableCode > model options > Resolve the correct number of GPU layers > attempts to resolve 32 gpuLayers
Error: Snapshot `stableCode > model options > Resolve the correct number of GPU layers > attempts to resolve 32 gpuLayers 1` mismatched

Expected: [32m"11[7m347[27m"[39m
Received: [31m"11[7m616[27m"[39m

 ‚ùØ test/modelDependent/stableCode/stableCodeModelGpuLayersOptions.test.ts:177:48
    175|                     });
    176|                     expect(res.gpuLayers).to.eql(32);
    177|                     expect(res.contextSize).to.toMatchInlineSnapshot("‚Ä¶
       |                                                ^
    178|                 }
    179|                 try {

‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ[12/15]‚éØ

 FAIL  test/modelDependent/stableCode/stableCodeModelGpuLayersOptions.test.ts > stableCode > model options > Resolve the correct number of GPU layers > attempts to resolve "auto"
Error: Snapshot `stableCode > model options > Resolve the correct number of GPU layers > attempts to resolve "auto" 4` mismatched

Expected: [32m"2"[39m
Received: [31m"4"[39m

 ‚ùØ test/modelDependent/stableCode/stableCodeModelGpuLayersOptions.test.ts:356:46
    354|                         freeVram: s1GB * 0.8
    355|                     });
    356|                     expect(res.gpuLayers).to.toMatchInlineSnapshot("2"‚Ä¶
       |                                              ^
    357|                     expect(res.contextSize).to.toMatchInlineSnapshot("‚Ä¶
    358|                 }

‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ[13/15]‚éØ

 FAIL  test/modelDependent/stableCode/stableCodeModelGpuLayersOptions.test.ts > stableCode > model options > Resolve the correct number of GPU layers > attempts to resolve {min?: number, max?: number}
Error: Snapshot `stableCode > model options > Resolve the correct number of GPU layers > attempts to resolve {min?: number, max?: number} 5` mismatched

Expected: [32m"1[7m3252[27m"[39m
Received: [31m"1[7m5939[27m"[39m

 ‚ùØ test/modelDependent/stableCode/stableCodeModelGpuLayersOptions.test.ts:507:48
    505|                     });
    506|                     expect(res.gpuLayers).to.eql(16);
    507|                     expect(res.contextSize).to.toMatchInlineSnapshot("‚Ä¶
       |                                                ^
    508|                 }
    509|                 try {

‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ[14/15]‚éØ

 FAIL  test/modelDependent/stableCode/stableCodeModelGpuLayersOptions.test.ts > stableCode > model options > Resolve the correct number of GPU layers > attempts to resolve {fitContext?: {contextSize?: number}}
Error: Snapshot `stableCode > model options > Resolve the correct number of GPU layers > attempts to resolve {fitContext?: {contextSize?: number}} 5` mismatched

Expected: [32m"3"[39m
Received: [31m"7"[39m

 ‚ùØ test/modelDependent/stableCode/stableCodeModelGpuLayersOptions.test.ts:577:46
    575|                         freeVram: s1GB * 1
    576|                     });
    577|                     expect(res.gpuLayers).to.toMatchInlineSnapshot("3"‚Ä¶
       |                                              ^
    578|                     expect(res.contextSize).to.toMatchInlineSnapshot("‚Ä¶
    579|                     expect(res.contextSize).to.be.gte(contextSize);

‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ[15/15]‚éØ


  Snapshots  5 failed
 Test Files  8 failed | 13 passed (21)
      Tests  15 failed | 36 passed | 10 skipped (61)
   Start at  09:04:51
   Duration  74.78s (transform 2.23s, setup 34ms, collect 6.76s, tests 65.28s, environment 0ms, prepare 2.07s)

